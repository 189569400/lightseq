# Performance

The following table is a comparison on a fr2en translation model which is a Transformer-big with a
beam size of 4 and a target vocabulary size of approximately 30k.
<table>
   <tr>
      <td>batch_size</td>
      <td>seq_len</td>
      <td>tf-fp32-p4, ms</td>
      <td>byseq-fp32-p4, ms</td>
      <td>byseq-fp16-t4, ms</td>
      <td>byseq-fp32-p4/tf-fp32-p4, speedup</td>
      <td>byseq-fp16-t4/byseq-fp32-p4, speedup</td>
      <td>byseq-fp16-t4/tf-fp32-p4, speedup</td>
   </tr>
   <tr>
      <td rowspan="8">1</td>
      <td>6</td>
      <td>303</td>
      <td>47</td>
      <td>27</td>
      <td>6.44</td>
      <td>1.74</td>
      <td>11.22</td>
   </tr>
   <tr>
      <td>12</td>
      <td>399</td>
      <td>63</td>
      <td>38</td>
      <td>6.33</td>
      <td>1.66</td>
      <td>10.5</td>
   </tr>
   <tr>
      <td>18</td>
      <td>702</td>
      <td>108</td>
      <td>59</td>
      <td>6.5</td>
      <td>1.83</td>
      <td>11.9</td>
   </tr>
   <tr>
      <td>24</td>
      <td>1071</td>
      <td>167</td>
      <td>82</td>
      <td>6.41</td>
      <td>2.04</td>
      <td>13.06</td>
   </tr>
   <tr>
      <td>36</td>
      <td>1234</td>
      <td>192</td>
      <td>105</td>
      <td>6.42</td>
      <td>1.83</td>
      <td>11.75</td>
   </tr>
   <tr>
      <td>46</td>
      <td>1445</td>
      <td>227</td>
      <td>110</td>
      <td>6.36</td>
      <td>2.06</td>
      <td>13.14</td>
   </tr>
   <tr>
      <td>58</td>
      <td>1887</td>
      <td>303</td>
      <td>142</td>
      <td>6.22</td>
      <td>2.13</td>
      <td>13.29</td>
   </tr>
   <tr>
      <td>70</td>
      <td>2771</td>
      <td>428</td>
      <td>197</td>
      <td>6.47</td>
      <td>2.17</td>
      <td>14.07</td>
   </tr>
   <tr>
      <td rowspan="8">2</td>
      <td>6</td>
      <td>317</td>
      <td>57</td>
      <td>32</td>
      <td>5.56</td>
      <td>1.78</td>
      <td>9.91</td>
   </tr>
   <tr>
      <td>12</td>
      <td>418</td>
      <td>73</td>
      <td>39</td>
      <td>5.72</td>
      <td>1.87</td>
      <td>10.72</td>
   </tr>
   <tr>
      <td>18</td>
      <td>723</td>
      <td>131</td>
      <td>66</td>
      <td>5.51</td>
      <td>1.98</td>
      <td>10.95</td>
   </tr>
   <tr>
      <td>24</td>
      <td>1113</td>
      <td>201</td>
      <td>91</td>
      <td>5.53</td>
      <td>2.21</td>
      <td>12.23</td>
   </tr>
   <tr>
      <td>36</td>
      <td>1276</td>
      <td>234</td>
      <td>104</td>
      <td>5.45</td>
      <td>2.25</td>
      <td>12.27</td>
   </tr>
   <tr>
      <td>46</td>
      <td>1521</td>
      <td>282</td>
      <td>121</td>
      <td>5.39</td>
      <td>2.33</td>
      <td>12.57</td>
   </tr>
   <tr>
      <td>58</td>
      <td>2004</td>
      <td>371</td>
      <td>159</td>
      <td>5.4</td>
      <td>2.33</td>
      <td>12.6</td>
   </tr>
   <tr>
      <td>70</td>
      <td>2965</td>
      <td>542</td>
      <td>221</td>
      <td>5.47</td>
      <td>2.45</td>
      <td>13.42</td>
   </tr>
   <tr>
      <td rowspan="8">4</td>
      <td>6</td>
      <td>326</td>
      <td>61</td>
      <td>39</td>
      <td>5.34</td>
      <td>1.56</td>
      <td>8.36</td>
   </tr>
   <tr>
      <td>12</td>
      <td>433</td>
      <td>85</td>
      <td>47</td>
      <td>5.09</td>
      <td>1.81</td>
      <td>9.21</td>
   </tr>
   <tr>
      <td>18</td>
      <td>761</td>
      <td>154</td>
      <td>77</td>
      <td>4.94</td>
      <td>2</td>
      <td>9.88</td>
   </tr>
   <tr>
      <td>24</td>
      <td>1195</td>
      <td>245</td>
      <td>113</td>
      <td>4.87</td>
      <td>2.17</td>
      <td>10.58</td>
   </tr>
   <tr>
      <td>36</td>
      <td>1391</td>
      <td>282</td>
      <td>128</td>
      <td>4.93</td>
      <td>2.2</td>
      <td>10.87</td>
   </tr>
   <tr>
      <td>46</td>
      <td>1679</td>
      <td>339</td>
      <td>153</td>
      <td>4.95</td>
      <td>2.22</td>
      <td>10.97</td>
   </tr>
   <tr>
      <td>58</td>
      <td>2232</td>
      <td>455</td>
      <td>199</td>
      <td>4.9</td>
      <td>2.29</td>
      <td>11.22</td>
   </tr>
   <tr>
      <td>70</td>
      <td>3406</td>
      <td>673</td>
      <td>285</td>
      <td>5.06</td>
      <td>2.36</td>
      <td>11.95</td>
   </tr>
   <tr>
      <td rowspan="8">8</td>
      <td>6</td>
      <td>364</td>
      <td>76</td>
      <td>43</td>
      <td>4.78</td>
      <td>1.77</td>
      <td>8.47</td>
   </tr>
   <tr>
      <td>12</td>
      <td>470</td>
      <td>110</td>
      <td>56</td>
      <td>4.27</td>
      <td>1.96</td>
      <td>8.39</td>
   </tr>
   <tr>
      <td>18</td>
      <td>854</td>
      <td>205</td>
      <td>91</td>
      <td>4.16</td>
      <td>2.25</td>
      <td>9.38</td>
   </tr>
   <tr>
      <td>24</td>
      <td>1381</td>
      <td>318</td>
      <td>139</td>
      <td>4.34</td>
      <td>2.29</td>
      <td>9.94</td>
   </tr>
   <tr>
      <td>36</td>
      <td>1628</td>
      <td>378</td>
      <td>156</td>
      <td>4.3</td>
      <td>2.42</td>
      <td>10.44</td>
   </tr>
   <tr>
      <td>46</td>
      <td>1989</td>
      <td>459</td>
      <td>193</td>
      <td>4.33</td>
      <td>2.38</td>
      <td>10.31</td>
   </tr>
   <tr>
      <td>58</td>
      <td>2683</td>
      <td>617</td>
      <td>254</td>
      <td>4.34</td>
      <td>2.43</td>
      <td>10.56</td>
   </tr>
   <tr>
      <td>70</td>
      <td>4251</td>
      <td>949</td>
      <td>382</td>
      <td>4.47</td>
      <td>2.48</td>
      <td>11.13</td>
   </tr>
</table>

The following table is a comparison on a en2zh translation model which is a
Transformer-deep(Compared with Transformer-big, it has 16 layers of encoder and other configurations
remain the same) with a
beam size of 4 and a target vocabulary size of approximately 30k.

<table>
   <tr>
      <td>batch_size</td>
      <td>seq_len</td>
      <td>tf-fp32-p4, ms</td>
      <td>byseq-fp32-p4, ms</td>
      <td>byseq-fp16-t4, ms</td>
      <td>byseq-fp32-p4/tf-fp32-p4, speedup</td>
      <td>byseq-fp16-t4/byseq-fp32-p4, speedup</td>
      <td>byseq-fp16-t4/tf-fp32-p4, speedup</td>
   </tr>
   <tr>
      <td rowspan="6">1</td>
      <td>12</td>
      <td>544</td>
      <td>86</td>
      <td>43</td>
      <td>6.32</td>
      <td>2</td>
      <td>12.65</td>
   </tr>
   <tr>
      <td>24</td>
      <td>914</td>
      <td>131</td>
      <td>66</td>
      <td>6.97</td>
      <td>1.98</td>
      <td>13.85</td>
   </tr>
   <tr>
      <td>36</td>
      <td>1290</td>
      <td>200</td>
      <td>93</td>
      <td>6.45</td>
      <td>2.15</td>
      <td>13.87</td>
   </tr>
   <tr>
      <td>48</td>
      <td>1836</td>
      <td>233</td>
      <td>106</td>
      <td>7.89</td>
      <td>2.2</td>
      <td>17.32</td>
   </tr>
   <tr>
      <td>72</td>
      <td>3456</td>
      <td>482</td>
      <td>212</td>
      <td>7.17</td>
      <td>2.27</td>
      <td>16.3</td>
   </tr>
   <tr>
      <td>84</td>
      <td>2626</td>
      <td>431</td>
      <td>193</td>
      <td>6.09</td>
      <td>2.23</td>
      <td>13.61</td>
   </tr>
   <tr>
      <td rowspan="6">2</td>
      <td>12</td>
      <td>566</td>
      <td>100</td>
      <td>50</td>
      <td>5.66</td>
      <td>2</td>
      <td>11.32</td>
   </tr>
   <tr>
      <td>24</td>
      <td>842</td>
      <td>158</td>
      <td>70</td>
      <td>5.32</td>
      <td>2.26</td>
      <td>12.03</td>
   </tr>
   <tr>
      <td>36</td>
      <td>1287</td>
      <td>247</td>
      <td>103</td>
      <td>5.21</td>
      <td>2.4</td>
      <td>12.5</td>
   </tr>
   <tr>
      <td>48</td>
      <td>1504</td>
      <td>288</td>
      <td>118</td>
      <td>5.22</td>
      <td>2.44</td>
      <td>12.75</td>
   </tr>
   <tr>
      <td>72</td>
      <td>3131</td>
      <td>611</td>
      <td>240</td>
      <td>5.12</td>
      <td>2.55</td>
      <td>13.05</td>
   </tr>
   <tr>
      <td>84</td>
      <td>2789</td>
      <td>546</td>
      <td>217</td>
      <td>5.1</td>
      <td>2.52</td>
      <td>12.85</td>
   </tr>
   <tr>
      <td rowspan="6">4</td>
      <td>12</td>
      <td>590</td>
      <td>118</td>
      <td>58</td>
      <td>5</td>
      <td>2.03</td>
      <td>10.17</td>
   </tr>
   <tr>
      <td>24</td>
      <td>885</td>
      <td>187</td>
      <td>89</td>
      <td>4.73</td>
      <td>2.1</td>
      <td>9.94</td>
   </tr>
   <tr>
      <td>36</td>
      <td>1380</td>
      <td>301</td>
      <td>127</td>
      <td>4.58</td>
      <td>2.37</td>
      <td>10.87</td>
   </tr>
   <tr>
      <td>48</td>
      <td>1622</td>
      <td>352</td>
      <td>149</td>
      <td>4.6</td>
      <td>2.36</td>
      <td>10.89</td>
   </tr>
   <tr>
      <td>72</td>
      <td>3492</td>
      <td>763</td>
      <td>311</td>
      <td>4.57</td>
      <td>2.45</td>
      <td>11.23</td>
   </tr>
   <tr>
      <td>84</td>
      <td>3145</td>
      <td>687</td>
      <td>282</td>
      <td>4.57</td>
      <td>2.44</td>
      <td>11.15</td>
   </tr>
   <tr>
      <td rowspan="6">8</td>
      <td>12</td>
      <td>631</td>
      <td>150</td>
      <td>66</td>
      <td>4.2</td>
      <td>2.27</td>
      <td>9.56</td>
   </tr>
   <tr>
      <td>24</td>
      <td>979</td>
      <td>248</td>
      <td>103</td>
      <td>3.94</td>
      <td>2.41</td>
      <td>9.5</td>
   </tr>
   <tr>
      <td>36</td>
      <td>1584</td>
      <td>412</td>
      <td>156</td>
      <td>3.84</td>
      <td>2.64</td>
      <td>10.15</td>
   </tr>
   <tr>
      <td>48</td>
      <td>1880</td>
      <td>477</td>
      <td>186</td>
      <td>3.94</td>
      <td>2.56</td>
      <td>10.11</td>
   </tr>
   <tr>
      <td>72</td>
      <td>4218</td>
      <td>1069</td>
      <td>404</td>
      <td>3.94</td>
      <td>2.65</td>
      <td>10.44</td>
   </tr>
   <tr>
      <td>84</td>
      <td>3831</td>
      <td>976</td>
      <td>373</td>
      <td>3.92</td>
      <td>2.62</td>
      <td>10.27</td>
   </tr>
</table>
